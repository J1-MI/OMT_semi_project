#!/usr/bin/env python3
"""
OMT OSINT Automation - Single file version
- Normalizes raw forum data
- Extracts IOCs (IP, domain, email, hash)
- Scores and exports to JSONL
- Input: .json, .jsonl, .csv, .html, .txt
"""

import os, re, json, csv, hashlib, argparse
from html.parser import HTMLParser

# -----------------------
# HTML Parser
class SimpleHTMLStripper(HTMLParser):
    def __init__(self):
        super().__init__()
        self.reset()
        self.text = []
    def handle_data(self, d):
        self.text.append(d)
    def get_text(self):
        return " ".join(self.text)

def strip_html(html_content):
    stripper = SimpleHTMLStripper()
    stripper.feed(html_content)
    return stripper.get_text()

# -----------------------
# IOC Extraction
IP_REGEX = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
DOMAIN_REGEX = r'\b(?:[a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}\b'
EMAIL_REGEX = r'\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-z]{2,}\b'
HASH_REGEX = r'\b[a-fA-F0-9]{32,128}\b'

def extract_iocs(text):
    return {
        'ips': re.findall(IP_REGEX, text),
        'domains': re.findall(DOMAIN_REGEX, text),
        'emails': re.findall(EMAIL_REGEX, text),
        'hashes': re.findall(HASH_REGEX, text),
    }

# -----------------------
# Scoring Function (Simple example)
def score_ioc(ioc_dict):
    score = 0
    if ioc_dict['ips']:
        score += 3
    if ioc_dict['domains']:
        score += 2
    if ioc_dict['emails']:
        score += 1
    if ioc_dict['hashes']:
        score += 5
    return score

# -----------------------
# Input Parsers
def parse_json(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def parse_csv(file_path):
    rows = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    return rows

def parse_txt(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return [{'text': f.read()}]

def parse_html(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        raw_html = f.read()
    return [{'text': strip_html(raw_html)}]

# -----------------------
# Main Processing
def process_file(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == '.json' or ext == '.jsonl':
        data = parse_json(file_path)
    elif ext == '.csv':
        data = parse_csv(file_path)
    elif ext == '.html':
        data = parse_html(file_path)
    else:
        data = parse_txt(file_path)

    results = []
    for item in data:
        text = item.get('text', str(item))
        iocs = extract_iocs(text)
        score = score_ioc(iocs)
        results.append({
            'text': text,
            'iocs': iocs,
            'score': score
        })
    return results

# -----------------------
# Export to JSONL
def export_jsonl(results, output_path):
    with open(output_path, 'w', encoding='utf-8') as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

# -----------------------
# CLI
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="OSINT Automation for Darkforum Data")
    parser.add_argument('input_file', help='Path to input data file')
    parser.add_argument('-o', '--output', default='osint_results.jsonl', help='Output JSONL file')
    args = parser.parse_args()

    processed = process_file(args.input_file)
    export_jsonl(processed, args.output)
    print(f"[+] Processed {len(processed)} items. Results saved to {args.output}")





----------- ▲ 코드 




▼ darkforum 사이트에서 직접 크롤링해서 나온 데이터들을 위 코드에서 어디에 넣어야 하는지 설명.

1. 예시: JSON 파일

크롤링한 데이터가 JSON이라면:

```json
[
  {"text": "Suspicious domain found: badsite[.]com and IP 192.168.1.5"},
  {"text": "Another email: attacker@example.com"}
]
```

여기서 `"text"` 필드 안에 포럼 글/댓글 내용을 넣으면 됨.

2.  예시: CSV 파일

크롤링한 데이터가 CSV라면:

```csv
text
"Suspicious domain found: badsite[.]com and IP 192.168.1.5"
"Another email: attacker@example.com"
```

3. 실행 방법

터미널에서 이렇게 실행하면 됩:

```bash
python osint_automation.py my_crawled_data.json -o results.jsonl
```

* `my_crawled_data.json` → 크롤링한 데이터 파일
* `results.jsonl` → 결과가 저장될 파일

4. 내부 동작

* 스크립트가 파일을 읽고
* 각 글/댓글 내용에서 IOC(IP, 도메인, 이메일, 해시)를 추출
* 점수 계산 후 JSONL로 저장






▼ 코드 설명

---

1. 파일 헤더

```python
#!/usr/bin/env python3
"""
OMT OSINT Automation - Single file version
- Normalizes raw forum data
- Extracts IOCs (IP, domain, email, hash)
- Scores and exports to JSONL
- Input: .json, .jsonl, .csv, .html, .txt
"""
```

* `#!/usr/bin/env python3`** : 스크립트를 터미널에서 바로 실행할 수 있도록 하는 shebang
* 주석: 이 스크립트의 목적과 기능 요약

  * 포럼 데이터 정규화
  * IOC(Indicator of Compromise) 추출
  * 간단 점수 계산
  * 결과를 `JSONL` 형식으로 저장

---

2. 모듈 임포트

```python
import os, re, json, csv, hashlib, argparse
from html.parser import HTMLParser
```

* `os` : 파일 경로 처리
* `re` : 정규표현식, IOC 추출
* `json` : JSON 읽기/쓰기
* `csv` : CSV 읽기
* `hashlib` : (현재 코드에서는 해시 계산용 준비, 실제 사용은 없음)
* `argparse` : CLI(커맨드라인 인터페이스) 처리
* `HTMLParser` : HTML 태그 제거용

---

3. HTML → 텍스트 변환

```python
class SimpleHTMLStripper(HTMLParser):
    def __init__(self):
        super().__init__()
        self.reset()
        self.text = []
    def handle_data(self, d):
        self.text.append(d)
    def get_text(self):
        return " ".join(self.text)

def strip_html(html_content):
    stripper = SimpleHTMLStripper()
    stripper.feed(html_content)
    return stripper.get_text()
```

* HTML 태그를 제거하고 텍스트만 추출
* 포럼 글이 HTML 형식으로 저장되어 있어도 깔끔하게 처리 가능

---

4. IOC(Indicator of Compromise) 추출

```python
IP_REGEX = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
DOMAIN_REGEX = r'\b(?:[a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}\b'
EMAIL_REGEX = r'\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-z]{2,}\b'
HASH_REGEX = r'\b[a-fA-F0-9]{32,128}\b'

def extract_iocs(text):
    return {
        'ips': re.findall(IP_REGEX, text),
        'domains': re.findall(DOMAIN_REGEX, text),
        'emails': re.findall(EMAIL_REGEX, text),
        'hashes': re.findall(HASH_REGEX, text),
    }
```

* 정규표현식으로 IP, 도메인, 이메일, 해시 추출
* `extract_iocs()` 함수는 텍스트를 입력받아 딕셔너리 형태로 결과 반환

---

5.  IOC 점수 계산

```python
def score_ioc(ioc_dict):
    score = 0
    if ioc_dict['ips']:
        score += 3
    if ioc_dict['domains']:
        score += 2
    if ioc_dict['emails']:
        score += 1
    if ioc_dict['hashes']:
        score += 5
    return score
```

* 간단한 위험도 점수 계산

  * IP 발견 → +3
  * 도메인 발견 → +2
  * 이메일 발견 → +1
  * 해시 발견 → +5

---

6. 파일 파서

```python
def parse_json(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def parse_csv(file_path):
    rows = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    return rows

def parse_txt(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return [{'text': f.read()}]

def parse_html(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        raw_html = f.read()
    return [{'text': strip_html(raw_html)}]
```

* 입력 파일 형식에 따라 읽는 방식이 다름
* 모든 형식의 데이터를 `{'text': 실제텍스트}` 형태로 통일

---

7. 메인 처리 함수

```python
def process_file(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == '.json' or ext == '.jsonl':
        data = parse_json(file_path)
    elif ext == '.csv':
        data = parse_csv(file_path)
    elif ext == '.html':
        data = parse_html(file_path)
    else:
        data = parse_txt(file_path)

    results = []
    for item in data:
        text = item.get('text', str(item))
        iocs = extract_iocs(text)
        score = score_ioc(iocs)
        results.append({
            'text': text,
            'iocs': iocs,
            'score': score
        })
    return results
```

* 파일 확장자에 맞는 파서 선택
* 각 글/댓글에서 IOC 추출 후 점수 계산
* 결과를 리스트로 반환

---

8. JSONL 저장

```python
def export_jsonl(results, output_path):
    with open(output_path, 'w', encoding='utf-8') as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
```

* 결과를 한 줄씩 JSON 형태로 저장
* JSONL → 여러 줄 JSON 구조, 데이터 처리 파이프라인에 편리

---

9. CLI 실행

```python
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="OSINT Automation for Darkforum Data")
    parser.add_argument('input_file', help='Path to input data file')
    parser.add_argument('-o', '--output', default='osint_results.jsonl', help='Output JSONL file')
    args = parser.parse_args()

    processed = process_file(args.input_file)
    export_jsonl(processed, args.output)
    print(f"[+] Processed {len(processed)} items. Results saved to {args.output}")
```

* 터미널에서 이렇게 실행 가능:

```bash
python osint_automation.py my_crawled_data.json -o results.jsonl
```

* `input_file` → 크롤링한 데이터
* `output` → 처리 결과

---

정리:

1. 크롤링 데이터 → JSON, CSV, TXT, HTML 중 하나로 저장
2. 스크립트 실행 → `process_file()`에서 읽어 IOC 추출 + 점수 계산
3. `JSONL` 결과로 저장 → 다른 OSINT 툴/분석 파이프라인에서 활용 가능

---

