코드에 맨 마지막 CLI 부분에 커맨드라인에서 인자 추가해서 실행.

-i : 크롤링한 데이터가 들어 있는 디렉토리
-o : 결과를 저장할 JSONL 파일 경로

예시: /Users/0000/Downloads/crawled_data → 크롤링한 데이터가 있는 폴더로 바꿔주기.

/Users/0000/Downloads/out.jsonl → 저장할 파일 경로와 이름 지정.


1. 파일 상단: 설명과 임포트

```python
#!/usr/bin/env python3
"""
OMT OSINT Automation - Single file version
- Crawled forum data -> normalize -> IOC extraction -> score -> JSONL export
- Supports input formats: .json, .jsonl, .csv, .html, .txt
- No external dependencies required
"""
import os, re, csv, json, hashlib, argparse, datetime, pathlib
from html.parser import HTMLParser
```

* 여러 타입의 입력 파일(`.json`, `.csv`, `.html` 등)을 처리.
* 외부 라이브러리 필요 없음.
* IOC(Indicators of Compromise) 추출과 점수 계산까지 수행.

---

2. HTML 파싱

```python
class SimpleHTMLStripper(HTMLParser):
    ...
def strip_html(path: str) -> str:
    ...
```

* HTML 파일에서 텍스트만 추출.
* `HTMLParser` 내장 모듈만 사용.
* 결과는 공백 단위로 정리(`split/join`)된 텍스트.

---

3. 유틸 함수

```python
def content_hash(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8", errors="ignore")).hexdigest()
```

* 게시글의 텍스트/제목을 SHA-256 해시로 변환.
* **중복 콘텐츠 제거용**.

```python
def guess_timestamp(value) -> str:
    ...
```

* 다양한 포맷의 날짜/시간 문자열을 ISO 8601 형식(`YYYY-MM-DDTHH:MM:SSZ`)으로 변환.

---

4. 입력 파일 탐색

```python
def walk_inputs(input_dir: str):
    ...
```

* 지정된 폴더 안의 모든 파일을 재귀적으로 탐색.
* `.json`, `.csv`, `.html`, `.txt` 등 다양한 파일을 지원.
* 각 파일을 레코드 단위로 읽어서 딕셔너리로 반환.

---

5. 정규화(Normalization)

```python
FIELD_MAP = { ... }
def normalize_record(raw):
    ...
```

* 서로 다른 파일/포맷에서 나온 필드 이름을 표준화.

  * 예: `threadId`, `post_id` → `thread_id`
  * `author`, `username` → `author`
  * `content_hash`와 `posted_at` 필드를 추가.
  * `_raw` 필드에 원본 데이터 보존.

---

6.IOC(Indicators of Compromise) 추출

```python
PATTERNS = { ... }
def extract_iocs(text):
    ...
```

* IPv4, 이메일, URL, BTC/ETH 주소, SHA256, CVE, 텔레그램/디스코드 계정 등 **보안 관련 패턴 추출.
* `re`(정규표현식)만 사용.

---

7. 키워드 스코어링

```python
KEYWORDS = [ ... ]
def keyword_hits(text):
    ...
def compute_score(text, iocs):
    ...
```

* "sell", "leak", "database", "ransomware" 같은 키워드가 텍스트에 있는지 체크.
* IOC 개수와 키워드 출현 횟수에 따라 점수 계산.
* 점수는 최대 100점.

---

8. 출력(JSONL)

```python
def write_jsonl(records, out_path):
    ...
```

* 결과를 \*\*한 줄에 한 레코드씩(JSONL)\*\*로 저장.
* `os.makedirs`로 경로 없으면 생성.

---

9. 메인 파이프라인

```python
def pipeline(input_dir, out_jsonl):
    seen = set()
    results = []
    for raw in walk_inputs(input_dir):
        rec = normalize_record(raw)
        text = rec.get("title", "") + "\n" + rec.get("text", "")
        iocs = extract_iocs(text)
        rec.update({"iocs": iocs, **compute_score(text, iocs)})
        h = rec.get("content_hash")
        if h in seen:
            continue
        seen.add(h)
        results.append(rec)
    write_jsonl(results, out_jsonl)
    return len(results)
```

* 파일 읽기 → 정규화 → IOC 추출 → 점수 계산 → 중복 제거 → JSONL 저장 단일 흐름.
* `seen` 집합으로 중복 콘텐츠 제거.

---

10. CLI(Command Line Interface)

```python
def main():
    ap = argparse.ArgumentParser(...)
    ap.add_argument("-i", "--input", required=True, help="Input directory with crawled files")
    ap.add_argument("-o", "--out", required=True, help="Output JSONL path")
    args = ap.parse_args()
    n = pipeline(args.input, args.out)
    print(f"[+] Wrote {n} records to {args.out}")
```

* 커맨드라인에서 실행 가능:

```bash
python osint_pipeline.py -i ./crawled_data -o ./out.jsonl
```

* 입력 디렉토리(`-i`) 안의 모든 지원 파일을 읽고,
* 하나의 JSONL 파일**로 합쳐 저장.
* 중복 콘텐츠 제거 및 IOC/점수 필드 추가.

---

정리

1. 입력: JSON, CSV, HTML, TXT 등
2. 처리: 정규화 → IOC 추출 → 점수 계산 → 중복 제거
3. 출력: JSONL 파일, 각 레코드마다 `iocs`, `score`, `keyword_hits` 포함
4. 특징:

   * 외부 라이브러리 필요 없음
   * 모든 JSON 파일 통합 가능
   * 포럼 크롤링 데이터 OSINT 자동화 가능

---



  
